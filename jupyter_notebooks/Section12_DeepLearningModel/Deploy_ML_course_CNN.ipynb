{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Read this before starting**\n\nThis notebook was adapted from the course materials for the Udemy course \"Deployment of Machine Learning Models\". For more info, see the README in the GitHub repo. \n- **Source data**: The notebook uses the Kaggle dataset \"V2 Plant Seedlings Dataset\" (<https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset>).\n- **On Kaggle**: To avoid downloading the large dataset locally, a Kaggle Kernel has been created to run this notebook here: <https://www.kaggle.com/btw78jt/deploy-ml-course-cnn>.\n- **In the Git repo**: The notebook is saved in my GitHub fork of the course repo here: <https://github.com/A-Breeze/deploying-machine-learning-models>. See `jupyter_notebooks/Section12_DeepLearningModel/`. It is up to the user to *manually* ensure that the copy of the notebook in the Kaggle Kernel is the same as the copy committed to the repo."},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning Model Building Pipeline: Big Data, Images and Neural Networks\n\nIn this notebook, we go through a practical example of how to build a Neural Network utilising a big dataset (> 1GB). We will do some data exploration, to understand what the dataset is about, and how we need to pre-process our data, to be able to use it in a convolutional neural network.\n\nThe accompanying repo goes on to show the the code for productionising and deployment of the model."},{"metadata":{},"cell_type":"markdown","source":"<!-- This table of contents is updated *manually* -->\n## Contents\n1. [Differentiating weed from crop seedlings](#Differentiating-weed-from-crop-seedlings)\n1. [Load images](#Load-images)\n1. [Examine images](#Examine-images)\n1. [Separate train and test](#Separate-train-and-test)\n1. [Pre-process data for modelling](#Pre-process-data-for-modelling)\n1. [CNN: Specify and train](#CNN:-Specify-and-train)\n1. [CNN: Assess model](#CNN:-Assess-model)\n"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></p>\n\n## Differentiating weed from crop seedlings\n\nThe aim of the project is to correctly identify the weed type from a variety of weed and crop RGB images.\n\n### Why is this important? \n\nAs taken from Kaggle website:\n\n\"Successful cultivation of maize depends largely on the efficacy of weed control. Weed control during the first six to eight weeks after planting is crucial, because weeds compete vigorously with the crop for nutrients and water during this period. Annual yield losses occur as a result of weed infestations in cultivated crops. Crop yield losses that are attributable to weeds vary with type of weed, type of crop, and the environmental conditions involved. Generally, depending on the level of weed control practiced yield losses can vary from 10 to 100 %. Thereore, effective weed control is imperative. In order to do effective control the first critical requirement is correct weed identification.\"\n\n\n### What is the objective of the machine learning model?\n\nWe aim to maximise the accuracy, this is, the correct classification of the different weed varieties.\n\n### How do I download the dataset?\n- Go to the Kaggle dataset page: <https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset>. Log in to Kaggle.\n- Click **download (2GB)** button towards the top right of the screen, to download the dataset.\n    -  You may need to accept terms and conditions of the competition.\n- Unzip the folder and save it in `NOTEBOOK_FOLDER/kaggle/input`, where `NOTEBOOK_FOLDER` is the directory of this notebook.\n\n===================================================================================================="},{"metadata":{"trusted":true},"cell_type":"code","source":"# check system that is running\nimport platform\nimport sys\n\n# Show all warnings in IPython\nimport warnings\nwarnings.filterwarnings(\"always\")\n# Ignore specific numpy warning, as per: <https://github.com/numpy/numpy/issues/11788#issuecomment-422846396>\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\nwarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n\n# navigate folders\nfrom glob import glob\nimport os\nfrom pathlib import Path\n\n# saving output (with a timestamp)\nimport pickle\n\n# other utils\nimport time\nimport datetime\nimport re\n\n# to handle datasets\nimport numpy as np\nimport pandas as pd\n\n# for plotting\nfrom matplotlib import __version__ as mpl_version\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to open the images\nimport cv2\n\n# to display all the columns of the dataframe in the notebook\npd.pandas.set_option('display.max_columns', None)\n\n# data preprocessing\nfrom sklearn import __version__ as sk_version\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# evaluate model and separate train and test\nfrom sklearn.metrics import confusion_matrix\n\n# Confirm expected versions (i.e. the versions running in the Kaggle Kernel)\nassert platform.python_version() == '3.6.6'\nprint(f\"Python version:\\t\\t{sys.version}\")\nassert pd.__version__ == '0.25.3'\nprint(f\"pandas version:\\t\\t{pd.__version__}\")\nassert np.__version__ == '1.18.2'\nprint(f\"numpy version:\\t\\t{np.__version__}\")\nassert mpl_version == '3.2.1'\nprint(f\"matplotlib version:\\t{mpl_version}\")\nassert sns.__version__ == '0.10.0'\nprint(f\"seaborn version:\\t{sns.__version__}\")\nassert cv2.__version__ == '4.2.0'\nprint(f\"cv2 version:\\t\\t{cv2.__version__}\")\nassert sk_version == '0.22.2.post1'\nprint(f\"sklearn version:\\t{sk_version}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ignore warnings that can show up, specific to Keras\nwarnings.filterwarnings(\n    \"ignore\", message=\"can't resolve package from __spec__ or __package__\")\nwarnings.filterwarnings(\n    \"ignore\", message=\"unclosed file <_io.TextIOWrapper name='/root/.keras/keras.json'\")\n\n# for the convolutional network\nfrom keras import __version__ as keras_version\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.preprocessing import image\nfrom keras.utils import np_utils\n\n# Confirm expected version\nassert keras_version == '2.3.1'\nprint(f\"keras version:\\t{keras_version}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Configuration variables\nNOTEBOOK_FOLDER = Path('/')  # Change this to the location of your notebook\nDATA_FOLDER = NOTEBOOK_FOLDER / 'kaggle' / 'input' / 'v2-plant-seedlings-dataset'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></p>\n\n## Load Images / Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# each weed class is in a dedicated folder\nprint('\\t'.join(os.listdir(DATA_FOLDER)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's walk over the directory structure, so we understand\n# how the images are stored\nmax_print_subfolders = 4\nmax_print_files_per_folder = 3\nsubfolder_counter = 0\nfor class_folder_path in DATA_FOLDER.iterdir():\n    subfolder_counter += 1\n    if subfolder_counter > max_print_subfolders:\n        print(str(DATA_FOLDER / '...') + \"more subfolders in this folder...\")\n        break\n    file_counter = 0\n    for image_path in class_folder_path.glob(\"*.png\"):\n        file_counter += 1\n        if file_counter > max_print_files_per_folder:\n            print(str(class_folder_path / '...') + \"more files in this folder...\\n\")\n            break\n        print(image_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's create a dataframe:\n# the dataframe stores the image file name in one column\n# and the class of the weed (the target) in the next column\nimages_df = pd.DataFrame.from_records([\n    (image_file_path.name, image_file_path.parent.name) for \n    image_file_path in DATA_FOLDER.glob(\"*/*.png\")  # Only look one subfolder down\n], columns=['image', 'target']).sort_values(['target', 'image'])\n\ndef get_image_file_path(images_row, DATA_FOLDER=DATA_FOLDER):\n    \"\"\"Get the file path from a row of images_df\"\"\"\n    return(DATA_FOLDER / images_row.target / images_row.image)\n\nimages_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many images do we have per class?\nimages_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></p>\n\n## Examine images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's isolate a path, for demo\n# we want to load the image in this path later\nimages_df.loc[0, :].agg(get_image_file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's visualise a few images\n# if the images you see in your notebook are not the same, don't worry\n\ndef plot_single_image(df, image_number):\n    im = cv2.imread(str(df.loc[image_number, :].agg(get_image_file_path)))\n    plt.title(df.loc[image_number, :].agg(lambda x: f\"{x.target}: {x.image}\"))\n    plt.imshow(im)\n    \nplot_single_image(images_df, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_single_image(images_df, 3000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_single_image(images_df, 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's go ahead and plot a bunch of our images together,\n# so we get e better feeling of how our images look like\n\ndef plot_for_class(df, label):\n    # function plots 9 images\n    nb_rows = 3\n    nb_cols = 3\n    fig, axs = plt.subplots(nb_rows, nb_cols, figsize=(10, 10))\n    n = 0\n    for i in range(0, nb_rows):\n        for j in range(0, nb_cols):\n            tmp = df[df['target'] == label]\n            tmp.reset_index(drop=True, inplace=True)\n            im = cv2.imread(str(tmp.loc[n,:].agg(get_image_file_path)))\n            axs[i, j].set_title(tmp.loc[n, :].agg(lambda x: f\"{x.target}: {x.image}\"))\n            axs[i, j].imshow(im)\n            n += 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_for_class(images_df, 'Cleavers')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plot_for_class(images_df, 'Maize')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_for_class(images_df, 'Common Chickweed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></p>\n\n## Separate train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    images_df.target + '/' + images_df.image, images_df.target,\n    test_size=0.20, random_state=101\n)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the indices of the training data are shuffled\n# this will cause problems later\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reset index, because later we iterate over row number\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n\n# reset index in target as well\ny_train.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)\n\nprint(X_train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.value_counts(normalize=True) - y_test.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of images within each class for\n# train should be (roughly) the same in the test set\nthresh = 1.2e-2\nassert (np.abs(\n    y_train.value_counts(normalize=True) - y_test.value_counts(normalize=True)\n) < thresh).all()\nprint(f'Proportions are within the threshold of: {thresh:.1%}\\n')\ny_train.value_counts(normalize=True).to_frame(\"Proportion of sample\") \\\n.style.format('{:.2%}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></p>\n\n## Pre-process data for modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's prepare the target\n# it is a multiclass classification, so we need to make \n# one hot encoding of the target\n\nencoder = LabelEncoder()\nencoder.fit(y_train)\n\ntrain_y = np_utils.to_categorical(encoder.transform(y_train))\ntest_y = np_utils.to_categorical(encoder.transform(y_test))\n\nprint(train_y.shape)\nprint('')\nprint(train_y[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The images in our folders, are all different sizes\n# For neural networks however, we need images in the same size\n# The images will all be resized to this size:\n\nIMAGE_SIZE = 150","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def im_resize(image_location, image_size=IMAGE_SIZE, DATA_FOLDER=DATA_FOLDER):\n    return(cv2.resize(\n        cv2.imread(str(DATA_FOLDER / image_location)),\n        (IMAGE_SIZE, IMAGE_SIZE)\n    ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tmp = im_resize(X_train[7])\ntmp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the shape of the datasets needs to be (n1, n2, n3, n4)\n# where n1 is the number of observations\n# n2 and n3 are image width and length\n# and n4 indicates that it is a color image, so 3 planes per image\n\ndef create_dataset(image_locations, **kwargs):\n    \"\"\"**kwargs: Additional arguments to im_resize()\"\"\"\n    return(np.array([\n        im_resize(image_location, **kwargs) for image_location in image_locations\n    ]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Took me approx: 45 secs\nx_train = create_dataset(X_train, image_size=IMAGE_SIZE)\nprint(f'Train Dataset Images shape: {x_train.shape}   size: {x_train.size:,}\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Took me approx: 15 secs\nx_test = create_dataset(X_test)\nprint(f'Train Dataset Images shape: {x_train.shape}   size: {x_train.size:,}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of different classes\ny_train.unique().shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></p>\n\n## CNN: Specify and train"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Specify the cnn\n# Source: https://www.kaggle.com/fmarazzi/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n\n# CNN structure parameters\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n                 input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(12, activation = \"softmax\"))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training parameters\nbatch_size = 10\nepochs = 8\nfilepath = \"model.h5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define callbacks to run after specific epochs\ncheckpoint = ModelCheckpoint(\n    filepath, monitor='accuracy', verbose=1, \n    save_best_only=True, mode='max'\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor='accuracy', factor=0.5, patience=1, \n    verbose=1, mode='max', min_lr=0.00001\n)\ncallbacks_list = [checkpoint, reduce_lr]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Fit model\n# Took me approx: 45 mins\n\nrun_this_command = False  # Set to False to avoid inadvertently running this command\nhistory_filename_base = \"fitting_history\"\nif run_this_command:\n    history = model.fit(\n        x=x_train, y=train_y,\n        batch_size=batch_size, \n        validation_split=10,\n        epochs=epochs,\n        verbose=2,\n        callbacks=callbacks_list\n    )\n    # Save history (with timestamp in the filename)\n    new_filename = \"{history_filename_base}.pkl\"\n    try:  # It is crucial this does not fail, so I also have written a backup\n        ts = time.time()\n        st = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d_%H%M%S')\n        new_filename = f\"{history_filename_base}_{st}.pkl\"\n    except Exception:\n        pass\n    with open(new_filename, \"wb\") as output_file:\n        pickle.dump(history, output_file)\nelse:\n    print(\"Command has *not* been run\\n\")\n    previous_files = [path for path in Path(os.getcwd()).glob(f\"{history_filename_base}_*.pkl\")]\n    if len(previous_files) == 0:\n        print(\"No previous files available. History not loaded.\\n\")\n    else:\n        with open(sorted(previous_files)[-1], 'rb') as input_file:\n            history = pickle.load(input_file)\n        print(\"Most recent history file reloaded.\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View fitting history\nacc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.title('Training loss')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.title('Training accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></p>\n\n## CNN: Assess model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# calculate predictions on test set\n# Took me approx: 20 secs\npredictions = model.predict_classes(x_test, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspect predictions\npredictions[:50]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the model has simply predicted that every observation is in **one** particular class. Something has gone wrong. As this is not crucial to the course, I have not gone back to debug it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get confusion matrix\ncnf_matrix = confusion_matrix(encoder.transform(y_test), predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dict to map back the numbers onto the classes\nencoding_dict = dict(zip(range(len(encoder.classes_)), encoder.classes_))\nencoding_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abbreviation_dict = {}\nfor code, label in encoding_dict.items():\n    label_words = re.split(r\"[\\s-]\", label)\n    if len(label_words) == 1:\n        abbreviation_dict[code] = label_words[0][:2]\n    else:\n        abbreviation_dict[code] = ''.join([label_word[0].upper() for label_word in label_words])\nabbreviation = pd.DataFrame.from_dict(\n    abbreviation_dict, columns=['abbrev'], orient='index'\n).sort_index()\nabbreviation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1)\nax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(abbreviation.abbrev)\nax.set_yticklabels(abbreviation.abbrev)\nplt.title('Confusion Matrix')\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\n#fig.savefig('Confusion matrix.png', dpi=300)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(encoder.transform(y_test), predictions, normalize=True, sample_weight=None)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(classification_report(encoder.transform(y_test), predictions))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"<p style=\"text-align: right\"><a href=\"#Contents\">Back to Contents</a></p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.cpu_count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(os.sched_getaffinity(0))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"clone35","language":"python","name":"clone35"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"742px","left":"0px","right":"1649px","top":"107px","width":"168px"},"toc_section_display":"block","toc_window_display":true}},"nbformat":4,"nbformat_minor":4}